{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "mDgbUHAGgjLW",
        "3RnN4peoiCZX",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "7AN1z2sKpx6M",
        "_-qAgymDpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitish6121999/Capstone-project-Bike-Sharing-Demand-prediction-project/blob/main/SUPERVISED_ML_BIKE_SHARING__PREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Description**   \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Supervised Machine Learning Regression\n",
        "##### **Project Title -** - Bike Sharing Demand Prediction\n",
        "##### **Contribution**    - Individual\n",
        "##### **Created by -**    - Nitish N Naik\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The goal of this project is to combine the historical bike usage patterns with\n",
        "the weather data in order to forecast bike rental demand.\n",
        "\n",
        "2. The dataset contains weather information (Temperature, Humidity, Windspeed,\n",
        "Visibility, Dewpoint, Solar radiation, Snowfall, Rainfall), the number of bikes rented per hour and date information.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/nitish6121999/Capstone-project-Bike-Sharing-Demand-prediction-project\n",
        "\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bike sharing systems have become increasingly popular in urban areas, providing an efficient and eco-friendly mode of transportation. To ensure optimal bike availability and meet customer demand, bike sharing companies **need accurate predictions** of bike rental demand at different locations and times."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ridge_regression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3bMmFQWWbWGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path= '/content/drive/MyDrive/CAPSTONE PROJECTS/Project supervised ML :Bike sharing prediction/dataset for project/SeoulBikeData.csv'\n",
        "\n",
        "bike_df=pd.read_csv(path, encoding='unicode_escape', parse_dates=[0])"
      ],
      "metadata": {
        "id": "CCm7aOlNf9Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bike_df\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "bike_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "bike_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical features:- Rented Bike Count, Hour , Temperatures, Humidity, Wind Speed, Visibility, Dew point Temperatures, solar radiation, rainfall, and snowfall.\n",
        "\n",
        "Categorical features :- Season, Holliday, Functioning day"
      ],
      "metadata": {
        "id": "0EboIJDf3wKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "bike_df.duplicated().sum()\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are no dupliacted values present.**"
      ],
      "metadata": {
        "id": "V9VOdcgTmJ6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "bike_df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There are no missing values present in the dataset**"
      ],
      "metadata": {
        "id": "ODjo839OmQnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(20,5))\n",
        "sns.heatmap(bike_df.isnull(), cbar=False, yticklabels=False, cmap='coolwarm')\n",
        "plt.xlabel('name of the columns')\n",
        "plt.title('location of missing values')"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There is no indication of the missing values in the above showed graph.**"
      ],
      "metadata": {
        "id": "YziwnlpRoxY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has more of the numerical columns and three of the categorical columns ,where in there is no problem for the duplicate values ,null values or the missing values  ."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "bike_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bike_df.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ðŸ”¶: Date :- year-month-day\n",
        "\n",
        "ðŸ”¶: Rented Bike count :- Count of bikes rented at each hour\n",
        "\n",
        "ðŸ”¶: Hour :- Hour of he day (0 to 23)\n",
        "\n",
        "ðŸ”¶: Temperature :-Temperature of the day in degree celsius\n",
        "\n",
        "ðŸ”¶: Humidity :- Humidity measurement in %\n",
        "\n",
        "ðŸ”¶: Windspeed :- windspeed in m/s\n",
        "\n",
        "ðŸ”¶: Visibility :-Visibility measurement around 10meter\n",
        "\n",
        "ðŸ”¶: Dew point temperature :- Dew point measurement in degree Celsius\n",
        "\n",
        "ðŸ”¶: Solar radiation :- Solar radiation measureent in MJ/m2 (i.e. Megajules per meter square)\n",
        "\n",
        "ðŸ”¶: Rainfall :- Rainfall measurement in mm\n",
        "\n",
        "ðŸ”¶: Snowfall :- Snowfall measurement in cm\n",
        "\n",
        "ðŸ”¶: Seasons :- Winter, Spring, Summer, Fall or Autumn\n",
        "\n",
        "ðŸ”¶: Holiday :- Holiday/No holiday\n",
        "\n",
        "ðŸ”¶: Functional Day :- No Func(Non Functional Hours), Fun(Functional hours)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "bike_df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "-IjbyCL5ufjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "bike_df['Year']=bike_df['Date'].map(lambda x: x.year).astype('object')\n",
        "bike_df['Month']=bike_df['Date'].dt.month_name()\n",
        "bike_df['Day']=bike_df['Date'].dt.day_name()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.head()\n"
      ],
      "metadata": {
        "id": "eQ8qxmLcujfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Since we have converted Date columns into three respective year, month, day columns.**\n",
        "**So no need of Date column in dataframe so we will drop it.**\n"
      ],
      "metadata": {
        "id": "G9WoBBLzvJGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.drop(columns=\"Date\", inplace=True)"
      ],
      "metadata": {
        "id": "Qtp9F5ZguxPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing \"Hour\" column -->'int' dtype into 'object' dtype\n",
        "\n",
        "bike_df['Hour']=bike_df['Hour'].astype(object)"
      ],
      "metadata": {
        "id": "v5amup2m49Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.info()"
      ],
      "metadata": {
        "id": "cJ8kF9J5vsy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of Numerical Features = 9\n",
        "\n",
        "Number of Categorical Features = 7."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 1: PAIRPLOT (for how the features are related with each other)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "sns.pairplot(bike_df, diag_kind= 'kde')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE can understand how data is distributed across the dataset"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Storing Numerical and Categorical columns separately**"
      ],
      "metadata": {
        "id": "yToMMycc6afE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_col= bike_df.describe().columns\n",
        "\n",
        "categorical_col= bike_df.describe(include=['object','category']).columns"
      ],
      "metadata": {
        "id": "tMpNZPIZ6Om4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_col"
      ],
      "metadata": {
        "id": "gRScMsOS7lXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(numeric_col)"
      ],
      "metadata": {
        "id": "13arr_w67z5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_col"
      ],
      "metadata": {
        "id": "vgucBHGT7oql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(categorical_col)"
      ],
      "metadata": {
        "id": "MTG-PVaS75AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chart - Analysis of Categorical columns**\n",
        "\n"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charts\n",
        "\n",
        "for col in categorical_col:\n",
        "  counts = bike_df[col].value_counts().sort_index()\n",
        "  fig = plt.figure(figsize=(10,8))\n",
        "  ax = fig.gca()\n",
        "  counts.plot.bar(ax=ax,color='orange')\n",
        "  ax.set_title(col+'  counts distribution')\n",
        "  ax.set_xlabel(col)\n",
        "  ax.set_ylabel('frequency')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Column chart could easily exlain the categorical columns all in one go ."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graphs\n",
        "\n",
        "1.  Season distribution is evenly distributed across all the four seasons\n",
        "\n",
        "2.  In Holiday columns ,most of the data is from the Non_holiday section\n",
        "\n",
        "3.  In Function column, most of the data is from the functioning day\n",
        "\n",
        "4.  Most of the data is from the year 2018."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to yearly data ,2018 have been the opening year for the demand in the bikes, from this it can be predicted that there will be a upward graphs for the coming years.\n",
        "\n",
        "Bikes are in demand for the functioning day and when there is no holidays , so the analysis could be helpful for the business to fulfill the need if there is  shortage of bikes in any of the loactions and for that particular time."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chart - Understanding how the categorical columns are related with the dependent variable on the hourly basis.**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical_col:\n",
        "  if col == \"Hour\":\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    plt.figure(figsize=(16,7))\n",
        "    sns.pointplot(x=bike_df['Hour'], y=bike_df['Rented Bike Count'], hue=bike_df[col])\n",
        "    plt.title(f'rented bike count for {col} wrt Hours')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "9F9wHgLEZrOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of the Rented bike count for different columns with respect to time .\n",
        "\n",
        "\n",
        "*   For Season column, there is a clear difference in the count of the Rented bike count in SUMMER(more count) and WINTER seasons(less count)\n",
        "\n",
        "*   During No holidays and Functioning days there is more count in the Rented bikes\n",
        "\n",
        "*   IN 2018 there is more usage of bikes compaered to 2017, may be because in 2018 most of them were aware of the rented bikes then in 2017\n",
        "\n",
        "*   In the Month columns ,the usage of bikes in the november ,december and january is very low then rest of the months , there is high usage in the summer months.\n",
        "\n",
        "\n",
        "*   In the week columns , only in the weekends there is lower line ,rest of the days are quite similar as those are the functiong days\n",
        "\n",
        "*   Considering all the columns ,the 8th Hour and 18th Hour are the peak time of the bike usage because of the office working hours\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "lp3LV_buYbuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart can explian the different columns wrt to the dependent variable and Hour columns"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of the Rented bike count for different columns with respect to time .\n",
        "\n",
        "\n",
        "*   For Season column, there is a clear difference in the count of the Rented bike count in SUMMER(more count) and WINTER seasons(less count)\n",
        "\n",
        "*   During No holidays and Functioning days there is more count in the Rented bikes\n",
        "\n",
        "*   IN 2018 there is more usage of bikes compaered to 2017, may be because in 2018 most of them were aware of the rented bikes then in 2017\n",
        "\n",
        "*   In the Month columns ,the usage of bikes in the november ,december and january is very low then rest of the months , there is high usage in the summer months.\n",
        "\n",
        "\n",
        "*   In the week columns , only in the weekends there is lower line ,rest of the days are quite similar as those are the functiong days\n",
        "\n",
        "*   Considering all the columns ,the 8th Hour and 18th Hour are the peak time of the bike usage because of the office working hours\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above gained insights clearly help positive business impact and also help tackle the negative impacts on business"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart -4 Feature Engineering on Hour column\n",
        "\n",
        "Since hour column contains 0 to 23 hours data so if we divide these hourly data into sessions i.e. [ morning , afternoon, evening, night ]. then it will be better to analyze when number of bike count is maximum and minimum."
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hour(h):\n",
        "\n",
        "  if h >= 7 and h <=10:\n",
        "    return 'Morning'\n",
        "\n",
        "  elif h>=11 and h<=16:\n",
        "    return 'Afternoon'\n",
        "\n",
        "  elif h>=17 and h<=22:\n",
        "    return 'Evening'\n",
        "\n",
        "  else:\n",
        "    return 'Night'"
      ],
      "metadata": {
        "id": "ko7d_hPQUKGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['Hour']=bike_df['Hour'].apply(hour)"
      ],
      "metadata": {
        "id": "SFMoG9EteCFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['Hour'].value_counts()\n",
        ""
      ],
      "metadata": {
        "id": "4RJAc81kUe5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking if some outliers present in categroical features wrt dependent variable."
      ],
      "metadata": {
        "id": "Kb4vfTW-UwU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chart  : To detect the outliers in the dataset**"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "n=1\n",
        "for col in categorical_col:\n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.subplot(6,2,n)\n",
        "  n+=1\n",
        "  print('\\n')\n",
        "  print('='*70,col,'='*70)\n",
        "  print('\\n')\n",
        "  sns.boxplot(x=bike_df[col],y=bike_df[\"Rented Bike Count\"])\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above box plot we detect some outliers and it can be removed by IQR method, but here in seoul dataset we are not removing outliers because in features these outliers are not causing much skewness to our data."
      ],
      "metadata": {
        "id": "Ga90_dBzV7sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To know the outliers in the dataset"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some the boxes in the columns have outliers ."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categoring the weekend and weekdays"
      ],
      "metadata": {
        "id": "oYOhL1qbKmIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df['week']=bike_df['Day'].apply(lambda x: 'weekend' if x=='sunday' and x=='saturday' else 'weekdays')\n",
        "bike_df.drop(columns=['Day'], inplace=True)\n",
        "bike_df.head()\n"
      ],
      "metadata": {
        "id": "cWVJAAyua0wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.info()"
      ],
      "metadata": {
        "id": "cPZfiLeZckAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df.shape"
      ],
      "metadata": {
        "id": "iNUEEJR8dHsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_col"
      ],
      "metadata": {
        "id": "-kjyqQcCetGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_col=categorical_col.drop('Day')"
      ],
      "metadata": {
        "id": "SECx89Yoe-fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_col\n"
      ],
      "metadata": {
        "id": "27RAYGWIfNNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chart : Data distribution Percentage**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "n=1\n",
        "plt.figure(figsize=(15,15))\n",
        "for col in categorical_col:\n",
        "  plt.subplot(4,2,n)\n",
        "  n=n+1\n",
        "  plt.pie(bike_df[col].value_counts(),labels = bike_df[col].value_counts().keys().tolist(),autopct='%.0f%%')\n",
        "  plt.title(col)\n",
        "  plt.tight_layout()\n",
        ""
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie chart displays the dataset distribution in percentage ."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is proper distirbution of the data ,there is no noisy distribution of the data."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Till here we have understood how Categorical columns data is distributed and the relation with dependent variable.**"
      ],
      "metadata": {
        "id": "j9UoeZcygi6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chart  : ANALYSE NUMERICAL COLUMNS"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_col"
      ],
      "metadata": {
        "id": "tIR_dLJtitKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "for col in numeric_col:\n",
        "    fig = plt.figure(figsize=(7, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = bike_df[col]\n",
        "    sns.histplot(data=bike_df,x=col ,ax = ax,color='Orange', kde=True)\n",
        "    plt.title(col + '  Distribution')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above graphs we can see that there are a lot of attributes which are positively and negatively distributed.\n",
        "\n",
        "so these types of features distribution will not give better results and will not give better understanding about model.\n",
        "\n",
        "so to get better result we need to normalize data by using transformations."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the methods which can deal with skewness of the data\n",
        "\n",
        "**square-root for moderate skew** :- sqrt(x) for positively skewed data, sqrt(max(x+1) - x) for negatively skewed data\n",
        "\n",
        "**log for greater skew** :- log10(x) for positively skewed data, log10(max(x+1) - x) for negatively skewed data\n",
        "\n",
        "**inverse for severe skew** :- 1/x for positively skewed data 1/(max(x+1) - x) for negatively skewed data\n",
        "\n",
        "**Linearity and heteroscedasticity** :- First try log transformation in a situation where the dependent variable starts to increase more rapidly with increasing independent variable values If your data does the opposite â€“ dependent variable values decrease more rapidly with increasing independent variable values â€“ you can first consider a square transformation.\n",
        "\n",
        "Above methods can provide good outcomes but we have to perform different transformation for different skewed data to make normaly distributed data.\n",
        "\n",
        "Instead of this long process we can use ***power transformation*** on features to make them in normal distribution to give better visualisation."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chart  : RELATION BETWEEN THE NUMERICAL COLUMNS AND DEPENDENT VARIABLE**\n"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "for col in numeric_col[1:]:    # since rented bike count column is not included in this for loop\n",
        "  fig = plt.figure(figsize=(9,6))\n",
        "  ax = fig.gca()\n",
        "  feature = bike_df[col]\n",
        "  correlation = feature.corr(bike_df['Rented Bike Count'])\n",
        "  plt.scatter(x=feature, y= bike_df['Rented Bike Count'])\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Rented Bike Count')\n",
        "  ax.set_title('Rented Bike Count vs '+col+ ' with the correlation value : '+str(correlation))\n",
        "  z= np.polyfit(bike_df[col],bike_df['Rented Bike Count'],1)\n",
        "  y_hat = np.poly1d(z)(bike_df[col])\n",
        "  plt.plot(bike_df[col],y_hat,'r--',lw=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the correlation of the columns wrt the dependent variable"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From above scatter plot or Regression plot shows some numeric features has positive correlation with dependent variable and some has negative correlation\n",
        "\n",
        "correlation of features with dependent feature :--\n",
        "\n",
        "positive correlation = Temperature, wind speed, Visibility, Dew point temperature, solar radiation.\n",
        "\n",
        "negative correlation = Humidity, Rainfall, snowfall"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chart : Correlation map**"
      ],
      "metadata": {
        "id": "DNqRY_RSNjCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(abs(bike_df.corr()), cmap='coolwarm', annot=True)"
      ],
      "metadata": {
        "id": "jwCFYKACNhwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this graph we are able to see that there is multicollinearity in temperature(Â°C) and dev point temperature(Â°C) column.\n",
        "\n",
        "so we need to remove multicollinearity because :- Multicollinearity reduces the precision of the estimated coefficients, which weakens the statistical power of our regression model."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to remove the multicollinearity. To remove we can use VIF method"
      ],
      "metadata": {
        "id": "UProQynWolBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "   vif = pd.DataFrame()\n",
        "   vif[\"variables\"] = X.columns\n",
        "   vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "   return(vif)"
      ],
      "metadata": {
        "id": "_81ObOf5oqid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(bike_df[[i for i in bike_df.describe().columns if i not in ['Rented Bike Count','Dew point temperature(Â°C)']]])"
      ],
      "metadata": {
        "id": "b9Ci9OhuJHPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since VIF ( Variance inflation factor ) in general, a VIF above 10 indicates high correlation and is cause for concern.\n",
        "\n",
        "If VIF <=3 means variables are less correlated and multicollinearity does not exist in the regression model.\n",
        "\n",
        "So in our data, features are less correlated with each other so it does not causes more concern\n",
        "\n"
      ],
      "metadata": {
        "id": "7Uk_Mx9-KoBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " using one hot encoding (get dummies) or Data encoding method.\n",
        "\n",
        "Since performing feature encoding on = [ Hour, Seasons, Holiday, Functioning Day, Month, Week ]"
      ],
      "metadata": {
        "id": "tsn08SPTLak3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1=pd.get_dummies(bike_df, drop_first=True, sparse=True )"
      ],
      "metadata": {
        "id": "Dop7-5dPLchk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.info()\n"
      ],
      "metadata": {
        "id": "4c8BwyruXld3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.head()"
      ],
      "metadata": {
        "id": "S5jyWbuDX_WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transforming the dependent variable ,because it is positively skewed"
      ],
      "metadata": {
        "id": "X-7qyXThd9eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "sns.distplot((bike_df1['Rented Bike Count']**2), color='red').set_title('Rented Bike count by square trnasformation')"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(np.sqrt(bike_df1['Rented Bike Count']), color='red').set_title('Rented Bike count by square trnasformation')"
      ],
      "metadata": {
        "id": "W6L--hUpeMdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Square root transformation gives us the normal distribution from positively skewed data for dependent vairable."
      ],
      "metadata": {
        "id": "0ui797hmeg7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***DIVIDING THE DATA AS INDEPENDENT AND DEPENDENT DATA ***"
      ],
      "metadata": {
        "id": "9Cd1-uq1fNj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEPENDENT VARIABLE   Y   = 'RENTED BIKE COUNT '\n",
        "\n",
        "INDEPENDENT VARIABLE X   = 'ALL THE VARIABLE EXCEPT RENTED BIKE COUNT'"
      ],
      "metadata": {
        "id": "bOAPvITIfufT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x= bike_df1.drop(columns=['Rented Bike Count'])\n",
        "x=x.drop(columns=['Dew point temperature(Â°C)'])"
      ],
      "metadata": {
        "id": "f_bBM0JifL3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=np.sqrt(bike_df1['Rented Bike Count'])"
      ],
      "metadata": {
        "id": "2B8Ms490hFbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPLITING THE DATA INTO TRAIN AND TEST DATA"
      ],
      "metadata": {
        "id": "vbZ6LoU2hiqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.20,random_state=10)"
      ],
      "metadata": {
        "id": "7j254R03hqKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain.shape,xtest.shape,ytrain.shape,ytest.shape"
      ],
      "metadata": {
        "id": "tnPrpMO2iRtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Appending all models parameters to the corrosponding list\n",
        "mean_absolut_error = []\n",
        "mean_sq_error=[]\n",
        "root_mean_sq_error=[]\n",
        "training_score =[]\n",
        "r2_list=[]\n",
        "adj_r2_list=[]\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "def score_metrix (model,X_train,X_test,Y_train,Y_test):\n",
        "\n",
        "  '''\n",
        "    train the model and gives mae, mse,rmse,r2,adj r2 score of the model\n",
        "\n",
        "  '''\n",
        "  #training the model\n",
        "  model.fit(X_train,Y_train)\n",
        "\n",
        "  # Training Score\n",
        "  training  = model.score(X_train,Y_train)\n",
        "  print(\"Training score  =\", training)\n",
        "\n",
        "  try:\n",
        "      # finding the best parameters of the model if any\n",
        "    print(f\"The best parameters found out to be :{model.best_params_} \\nwhere model best score is:  {model.best_score_} \\n\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "  #predicting the Test set and evaluting the models\n",
        "\n",
        "  if model == LinearRegression() or model == Lasso() or model == Ridge():\n",
        "    Y_pred = model.predict(X_test)\n",
        "\n",
        "    #finding mean_absolute_error\n",
        "    MAE  = mean_absolute_error(Y_test**2,Y_pred**2)\n",
        "    print(\"MAE :\" , MAE)\n",
        "\n",
        "    #finding mean_squared_error\n",
        "    MSE  = mean_squared_error(Y_test**2,Y_pred**2)\n",
        "    print(\"MSE :\" , MSE)\n",
        "\n",
        "    #finding root mean squared error\n",
        "    RMSE = np.sqrt(MSE)\n",
        "    print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "    #finding the r2 score\n",
        "\n",
        "    r2 = r2_score(Y_test**2,Y_pred**2)\n",
        "    print(\"R2 :\" ,r2)\n",
        "    #finding the adjusted r2 score\n",
        "    adj_r2=1-(1-r2_score(Y_test**2,Y_pred**2))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "    print(\"Adjusted R2 : \",adj_r2,'\\n')\n",
        "\n",
        "  else:\n",
        "    # for tree base models\n",
        "    Y_pred = model.predict(X_test)\n",
        "\n",
        "    #finding mean_absolute_error\n",
        "    MAE  = mean_absolute_error(Y_test,Y_pred)\n",
        "    print(\"MAE :\" , MAE)\n",
        "\n",
        "    #finding mean_squared_error\n",
        "    MSE  = mean_squared_error(Y_test,Y_pred)\n",
        "    print(\"MSE :\" , MSE)\n",
        "\n",
        "    #finding root mean squared error\n",
        "    RMSE = np.sqrt(MSE)\n",
        "    print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "    #finding the r2 score\n",
        "\n",
        "    r2 = r2_score(Y_test,Y_pred)\n",
        "    print(\"R2 :\" ,r2)\n",
        "    #finding the adjusted r2 score\n",
        "    adj_r2=1-(1-r2_score(Y_test,Y_pred))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1))\n",
        "    print(\"Adjusted R2 : \",adj_r2,'\\n')\n",
        "\n",
        "    try:\n",
        "\n",
        "      # ploting the graph of feature importance\n",
        "\n",
        "      best = model.best_estimator_\n",
        "      features = X_train.columns\n",
        "      importances = best.feature_importances_\n",
        "      indices = np.argsort(importances)\n",
        "      plt.figure(figsize=(10,15))\n",
        "      plt.title('Feature Importance')\n",
        "      plt.barh(range(len(indices)), importances[indices], color='red', align='center')\n",
        "      plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "      plt.xlabel('Relative Importance')\n",
        "      plt.show()\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  # Here we appending the parameters for all models\n",
        "  mean_absolut_error.append(MAE)\n",
        "  mean_sq_error.append(MSE)\n",
        "  root_mean_sq_error.append(RMSE)\n",
        "  training_score.append(training)\n",
        "  r2_list.append(r2)\n",
        "  adj_r2_list.append(adj_r2)\n",
        "\n",
        "  print('*'*80)\n",
        "  # print the cofficient and intercept of which model have these parameters and else we just pass them\n",
        "  try :\n",
        "    print(\"coefficient \\n\",model.coef_)\n",
        "    print('\\n')\n",
        "    print(\"Intercept  = \" ,model.intercept_)\n",
        "  except:\n",
        "    pass\n",
        "  print('\\n')\n",
        "  print('*'*20, 'ploting the graph of Actual and predicted only with 80 observation', '*'*20)\n",
        "\n",
        "  # ploting the graph of Actual and predicted only with 80 observation for better visualisation which model have these parameters and else we just pass them\n",
        "  try:\n",
        "    # ploting the line graph of actual and predicted values\n",
        "    plt.figure(figsize=(15,7))\n",
        "    plt.plot((Y_pred)[:80])\n",
        "    plt.plot((np.array(Y_test)[:80]))\n",
        "    plt.legend([\"Predicted\",\"Actual\"])\n",
        "    plt.show()\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "pjX82ykis8dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 1: LINEAR REGRESSION MODEL"
      ],
      "metadata": {
        "id": "RT_tvf5DcFVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_metrix (LinearRegression(),xtrain,xtest,ytrain,ytest)"
      ],
      "metadata": {
        "id": "CknySa0Qpl7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above linear regression model shows quite good results.\n",
        "\n",
        "----> By observing training score we say that model is quite overfit.\n",
        "\n",
        "----> We can increase accuracy of model by scaling or transforming the training data by either min max scaler or standard scalar or Power transformer."
      ],
      "metadata": {
        "id": "Qh8zzn-BXPGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model - 2 Linear regression with Powertransformer"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Power transformations are very useful when we have to deal with skewed features and our model is sensitive to the symmetry of the distributions.\n",
        "\n",
        "The fit(data) method is used to compute the mean and std dev for a given feature to be used further for scaling. The transform(data) method is used to perform scaling using mean and std dev calculated using the . fit() method."
      ],
      "metadata": {
        "id": "uWJNQBq3XRNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "pt = PowerTransformer()\n",
        "xtrain_trans = pt.fit_transform(xtrain)      # fit transform the training set\n",
        "xtest_trans = pt.transform(xtest)"
      ],
      "metadata": {
        "id": "TFKIzUxIc7ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_metrix(LinearRegression(),xtrain_trans,xtest_trans,ytrain,ytest)"
      ],
      "metadata": {
        "id": "fdGN3U4fd1vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model - 3 Linear regression with Standardscaler"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler removes the mean and scales each feature/variable to unit variance. This operation is performed feature-wise in an independent way. StandardScaler can be influenced by outliers (if they exist in the dataset) since it involves the estimation of the empirical mean and standard deviation of each feature."
      ],
      "metadata": {
        "id": "vzcZ6I2R-6Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sl = StandardScaler()\n",
        "xtrain_strans = sl.fit_transform(xtrain)      # fit transform the training set\n",
        "xtest_strans = sl.transform(xtest)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_metrix(LinearRegression(),xtrain_strans,xtest_strans,ytrain,ytest)"
      ],
      "metadata": {
        "id": "26dKkw22f1Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4 :Linear Regression with polynomial features"
      ],
      "metadata": {
        "id": "qrnQywEEhNoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial regression is a kind of linear regression in which the relationship shared between the dependent and independent variables Y and X is modeled as the nth degree of the polynomial."
      ],
      "metadata": {
        "id": "9siF85HV_Rwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(2)                               #creating variable with degree 2\n",
        "poly_xtrain = poly.fit_transform(xtrain_trans)                # fit the train set\n",
        "poly_xtest = poly.transform(xtest_trans)                    #transform the test set\n",
        ""
      ],
      "metadata": {
        "id": "QPFQcbw-hNEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_metrix(LinearRegression(),poly_xtrain,poly_xtest,ytrain,ytest)"
      ],
      "metadata": {
        "id": "7j1vub2MfXbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Regularization**\n",
        "\n",
        "Regularization is one of the most important concepts of machine learning. It is a technique to prevent the model from overfitting by adding extra information to it.\n",
        "\n",
        "Two techniques of regularization are = **1) Lasso (l1 norm) and 2) Ridge regression (L2 norm)**"
      ],
      "metadata": {
        "id": "UFmfvwYI_q_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4: Ridge Regression"
      ],
      "metadata": {
        "id": "YoxPMSsNiBpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L2 = Ridge()\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100,0.5,1.5,1.6,1.7,1.8,1.9]}      # giving parameters\n",
        "L2_cv = GridSearchCV(L2, parameters, scoring='r2', cv=5)                                                                    #using gridsearchcv and cross validate the model\n",
        "score_metrix(L2_cv,xtrain_trans,xtest_trans,ytrain,ytest)"
      ],
      "metadata": {
        "id": "Z59xCLUoiJPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model : Lasso regression"
      ],
      "metadata": {
        "id": "zWLX8mnGjuYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L1 = Lasso() #creating variable\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,0.0014]} #lasso parameters\n",
        "lasso_cv = GridSearchCV(L1, parameters, cv=5) #using gridsearchcv and cross validate the model"
      ],
      "metadata": {
        "id": "BEK8z6TViRzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_metrix(lasso_cv,xtrain_trans,xtest_trans,ytrain,ytest)"
      ],
      "metadata": {
        "id": "QL2ZtBr7j9MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model : RANDOM FORREST REGRESSION"
      ],
      "metadata": {
        "id": "7540LTARQhSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_x=bike_df1.drop(columns='Rented Bike Count')\n",
        "new_y=bike_df1['Rented Bike Count']"
      ],
      "metadata": {
        "id": "1v54UU8sc0R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_xtrain, new_xtest, new_ytrain, new_ytest = train_test_split(new_x, new_y, test_size= 0.20, random_state = 10)\n",
        "\n",
        "\n",
        "new_xtrain.shape,   new_xtest.shape,   new_ytrain.shape,  new_ytest.shape"
      ],
      "metadata": {
        "id": "9vYXc4vOcuLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "# parameters for random forest regression model\n",
        "\n",
        "rf_param_grid ={\"n_estimators\":[50,100,150],                    ### we can put any values for parameters\n",
        "              'max_depth' : [10,15,20,25,'none'],\n",
        "              'min_samples_split': [10,50,100],\n",
        "              'max_features' :[24,35,40,49]}\n",
        "\n",
        "\n",
        "# Using grid search cv\n",
        "Ranom_forest_Grid_search = GridSearchCV(RandomForestRegressor(),param_grid=rf_param_grid,n_jobs=-1,verbose=2)\n",
        ""
      ],
      "metadata": {
        "id": "mn3HSzMzWM_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_metrix(Ranom_forest_Grid_search, new_xtrain, new_xtest, new_ytrain, new_ytest)"
      ],
      "metadata": {
        "id": "7Ndc9u-IXBXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model : DECISION TREE REGRESSION"
      ],
      "metadata": {
        "id": "JJoPWRoyQh5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Include all the columns\n",
        "\n",
        "Since in decission tree multicollinearity of features does not affect the model accuracy. So in previous models we have removed multicollinear features (such as \"Dew Point Temperature\").\n"
      ],
      "metadata": {
        "id": "Zsp3VbuyQiTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_df1.shape"
      ],
      "metadata": {
        "id": "lqdswldgRVJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "\n",
        "# Parameters for Decission Tree model\n",
        "param_grid = {'criterion' : [\"squared_error\"],\n",
        "              'splitter' : [\"best\", \"random\"],\n",
        "              'max_depth' : [10,15,25, 'none'],\n",
        "              'min_samples_split': [10,50,100],\n",
        "              'max_features' :[24,35,40,49]}\n",
        "\n",
        "# Gridsearch CV\n",
        "Dt_grid_search = GridSearchCV(DecisionTreeRegressor(),param_grid=param_grid,cv=2,n_jobs=-1)"
      ],
      "metadata": {
        "id": "2CDGvg4yRaAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_metrix(Dt_grid_search,new_xtrain,new_xtest,new_ytrain,new_ytest)"
      ],
      "metadata": {
        "id": "Ph9w5z1zSZJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA Insights :-**\n",
        "\n",
        "1)  Bikes have been used more often in weekdays than weekends\n",
        "\n",
        "2) 98% of the bikes are rented when there is non Holiday day present. That means Most of the user may use bike on rent to go there respective work places.\n",
        "\n",
        "3) More number of bikes are rented in the Summer season and the lowest in the winter season.\n",
        "\n",
        "4)- Most number of bikes are rented when there is no snowfall or rainfall.\n",
        "\n",
        "5)- Peak rise in 8th and 18th hour\n",
        "\n",
        "6)- Gradual Increase in bike rent count is in morning 6 to 10 am i.e. it must be working time of employees. And after 10 am there slight decrease in count, And again start increasing count rate from 16 to 20 (4pm to 8pm) i.e. it must be leaving time of employees and they uses bike on rent to go there home.\n",
        "\n",
        "7)- The highest number of bike rentals have been done in the 18th hour, i.e 6pm, and lowest in the 4th hour, i.e 4am.\n",
        "\n",
        "8)- Most of the bike rentals have been made when there is high visibility.\n",
        "\n",
        "9)- In 2018 demand for Rented bikes is increased as compare to 2017 year. It may be because in 2017 people are aware about rented bike facility."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ML MODEL INSIGHTS**\n",
        "\n",
        "# Linear regression with original x train test and y train test\n",
        "\n",
        "    Training score  = 0.7025535504935366\n",
        "    MAE : 5.272386430820542\n",
        "    MSE : 44.56236594756176\n",
        "    RMSE : 6.675504920795262\n",
        "    R2 : 0.7148244596355067\n",
        "    Adjusted R2 :  0.710358253376898\n",
        "\n",
        "\n",
        "# **Linear regression with powertransformer**\n",
        "\n",
        "    Training score  = 0.7350893725883549\n",
        "    MAE : 5.081760981637516\n",
        "    MSE : 41.82562872623285\n",
        "    RMSE : 6.467273670275045\n",
        "    R2 : 0.7323381283856463\n",
        "    Adjusted R2 :  0.7281462081225445\n",
        "\n",
        "# **Linear regression with Standardscaler**\n",
        "    Training score  = 0.7025535504935366\n",
        "    MAE : 5.272386430820545\n",
        "    MSE : 44.56236594756178\n",
        "    RMSE : 6.675504920795264\n",
        "    R2 : 0.7148244596355066\n",
        "    Adjusted R2 :  0.710358253376898\n",
        "\n",
        "# **Linear regression with Polynomial features**\n",
        "    Training score  = 0.8409849039057424\n",
        "    MAE : 3.7984606042854523\n",
        "    MSE : 26.260078443291366\n",
        "    RMSE : 5.124458843945511\n",
        "    R2 : 0.8319494061672605\n",
        "    Adjusted R2 :  0.7812218663188648\n",
        "\n",
        "#**Ridge Regression**\n",
        "\n",
        "The best parameters found out to be :{'alpha': 10}\n",
        "\n",
        "where model best score is:  0.7321484459424913\n",
        "\n",
        "    Training score  = 0.7350857300036486\n",
        "    MAE : 5.081672261352577\n",
        "    MSE : 41.81620442680659\n",
        "    RMSE : 6.466545014674111\n",
        "    R2 : 0.7323984389105542\n",
        "    Adjusted R2 :  0.7282074631858355     \n",
        "\n",
        "#**Lasso Regression**\n",
        "\n",
        "\n",
        "The best parameters found out to be :{'alpha': 0.01}\n",
        "\n",
        "where model best score is:  0.7321939783512486\n",
        "\n",
        "    Training score  = 0.7350625551639408\n",
        "    MAE : 5.079811518964977\n",
        "    MSE : 41.802365832489144\n",
        "    RMSE : 6.465474911596916\n",
        "    R2 : 0.7324869985848095\n",
        "    Adjusted R2 :  0.7282974098155461\n",
        "\n",
        "#**Randomforrest Regression**\n",
        "\n",
        "\n",
        "The best parameters found out to be :{'max_depth': 25, 'max_features': 24, 'min_samples_split': 10, 'n_estimators': 150}\n",
        "\n",
        "where model best score is:  0.8237031376924666\n",
        "\n",
        "    Training score  = 0.9416053146851839\n",
        "    MAE : 169.6591645191808\n",
        "    MSE : 68890.38858227018\n",
        "    RMSE : 262.4697860369269\n",
        "    R2 : 0.8303437741263449\n",
        "    Adjusted R2 :  0.8275867373739001\n",
        "\n",
        "\n",
        "#**Decision Tree Regression**\n",
        "\n",
        "\n",
        "The best parameters found out to be :{'criterion': 'squared_error', 'max_depth': 10, 'max_features': 35, 'min_samples_split': 50, 'splitter': 'best'}\n",
        "\n",
        "where model best score is:  0.7589229343218231\n",
        "\n",
        "    Training score  = 0.8303549823331889\n",
        "    MAE : 189.78425743618234\n",
        "    MSE : 82961.48773177019\n",
        "    RMSE : 288.03035904531\n",
        "    R2 : 0.7956909056387869\n",
        "    Adjusted R2 :  0.7923707346334973\n",
        "\n"
      ],
      "metadata": {
        "id": "spMHXBn3aAF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Observing Insights of models we can conclude that :-\n",
        "\n",
        "1)- Random Forest Regression is the best model with an increased accuracy to predict bike rent count. i.e. R2 score of 0.83034....\n",
        "\n",
        "2)- Linear reagression model is the worst performing model with an r2 score of 0.71035...\n",
        "\n",
        "Actual vs Prediction visualisation is done for all the 6 models.\n",
        "\n",
        " Temperature and Hour are the two most important factors according to all the models. And they are very useful while predicting the bike rented count."
      ],
      "metadata": {
        "id": "0wGiyxPYejmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}